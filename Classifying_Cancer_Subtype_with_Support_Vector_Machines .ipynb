{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Cancer Subtype with Support Vector Machines \n",
    "# Our Problem \n",
    "- Doctors diagnose what kind of cancer their patients have using genetic tests. Can we use data to help doctors classify cancer type? \n",
    "- multiclass classification rather than binary \n",
    "- **class boundaries may not be linear**\n",
    "- use Support Vector Machines (SVM)\n",
    "\n",
    "![alt text](svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do SVMs work? \n",
    "\n",
    "### A simple example: maximal margin hyperplane \n",
    "\n",
    "- a maximal margin hyperplane is a line that has the farthest minimum distance to the training observations.\n",
    "- This allows us to separate observations into different classes \n",
    "![alt text](svm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending our simple example: using kernels \n",
    "- A kernel calculates the similarity of two observations. \n",
    "- A linear kernel will give us a decision boundary like the one above. \n",
    "- A **polynomial kernel** will give us a more flexible decision boundary. Hence we can apply the kernel to non-linear data. \n",
    "- Kernels are only restricted to polynomials. Below is a **radial kernel** \n",
    "\n",
    "![alt text](svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Dataset \n",
    "\n",
    "- Khan dataset: tissue samples corresponding to four types of cell tumours  \n",
    "\n",
    "- Each sample contains gene expression measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'xtrain'</li>\n",
       "\t<li>'xtest'</li>\n",
       "\t<li>'ytrain'</li>\n",
       "\t<li>'ytest'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'xtrain'\n",
       "\\item 'xtest'\n",
       "\\item 'ytrain'\n",
       "\\item 'ytest'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'xtrain'\n",
       "2. 'xtest'\n",
       "3. 'ytrain'\n",
       "4. 'ytest'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"xtrain\" \"xtest\"  \"ytrain\" \"ytest\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(ISLR)\n",
    "library(MASS)\n",
    "library(caret)\n",
    "names(Khan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features \n",
    "\n",
    "#### - We have gene expression measurements for 2308 genes. The training set has 63 observations, while the test set has 20 observations  \n",
    "#### - A dataset with many features like this is an example of a high-dimensional dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>63</li>\n",
       "\t<li>2308</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 63\n",
       "\\item 2308\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 63\n",
       "2. 2308\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]   63 2308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(Khan$xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>20</li>\n",
       "\t<li>2308</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 20\n",
       "\\item 2308\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 20\n",
       "2. 2308\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]   20 2308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(Khan$xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are our datasets balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  2  3  4 \n",
       " 8 23 12 20 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(Khan$ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>3</li>\n",
       "\t<li>2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3\n",
       "2. 2\n",
       "3. 4\n",
       "4. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3 2 4 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(Khan$ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a support vector approach to predict cancer subtype using gene expression measurements. In this data set, there are a very large number of features relative to the number of observations. This suggests that we should use a linear kernel, because the additional flexibility that will result from using a polynomial or radial kernel is unnecessary.\n",
    "\n",
    "This time, we will use the e1071 package that is included in caret, to show how to implement analysis outside of caret. We will place the x variables and y variables together into one data frame, then apply the svm( ) function to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>x.1</th><th scope=col>x.2</th><th scope=col>x.3</th><th scope=col>x.4</th><th scope=col>x.5</th><th scope=col>x.6</th><th scope=col>x.7</th><th scope=col>x.8</th><th scope=col>x.9</th><th scope=col>x.10</th><th scope=col>⋯</th><th scope=col>x.2300</th><th scope=col>x.2301</th><th scope=col>x.2302</th><th scope=col>x.2303</th><th scope=col>x.2304</th><th scope=col>x.2305</th><th scope=col>x.2306</th><th scope=col>x.2307</th><th scope=col>x.2308</th><th scope=col>y</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>V1</th><td> 0.77334370</td><td>-2.438405  </td><td>-0.4825622 </td><td>-2.7211350 </td><td>-1.2170580 </td><td> 0.82780920</td><td>1.342604   </td><td> 0.05704174</td><td> 0.13356890</td><td>0.5654274  </td><td>⋯          </td><td>-0.02747398</td><td>-1.660205  </td><td> 0.58823100</td><td>-0.46362400</td><td>-3.952845  </td><td>-5.496768  </td><td>-1.414282  </td><td>-0.6476004 </td><td>-1.76317200</td><td>2          </td></tr>\n",
       "\t<tr><th scope=row>V2</th><td>-0.07817778</td><td>-2.415754  </td><td> 0.4127717 </td><td>-2.8251460 </td><td>-0.6262365 </td><td> 0.05448819</td><td>1.429498   </td><td>-0.12024860</td><td> 0.45679170</td><td>0.1590529  </td><td>⋯          </td><td>-0.24628420</td><td>-0.836325  </td><td>-0.57128360</td><td> 0.03478783</td><td>-2.478130  </td><td>-3.661264  </td><td>-1.093923  </td><td>-1.2093200 </td><td>-0.82439550</td><td>2          </td></tr>\n",
       "\t<tr><th scope=row>V3</th><td>-0.08446916</td><td>-1.649739  </td><td>-0.2413075 </td><td>-2.8752860 </td><td>-0.8894054 </td><td>-0.02747398</td><td>1.159300   </td><td> 0.01567648</td><td> 0.19194180</td><td>0.4965847  </td><td>⋯          </td><td> 0.02498525</td><td>-1.059872  </td><td>-0.40376660</td><td>-0.67865270</td><td>-2.939352  </td><td>-2.736450  </td><td>-1.965399  </td><td>-0.8058680 </td><td>-1.13943400</td><td>2          </td></tr>\n",
       "\t<tr><th scope=row>V4</th><td> 0.96561400</td><td>-2.380547  </td><td> 0.6252965 </td><td>-1.7412560 </td><td>-0.8453664 </td><td> 0.94968680</td><td>1.093801   </td><td> 0.81973580</td><td>-0.28462010</td><td>0.9947322  </td><td>⋯          </td><td> 0.35711480</td><td>-1.893128  </td><td> 0.25510720</td><td> 0.16330860</td><td>-1.021929  </td><td>-2.077843  </td><td>-1.127629  </td><td> 0.3315315 </td><td>-2.17948300</td><td>2          </td></tr>\n",
       "\t<tr><th scope=row>V5</th><td> 0.07566390</td><td>-1.728785  </td><td> 0.8526265 </td><td> 0.2726953 </td><td>-1.8413700 </td><td> 0.32793590</td><td>1.251219   </td><td> 0.77144990</td><td> 0.03091710</td><td>0.2783133  </td><td>⋯          </td><td> 0.06175340</td><td>-2.273998  </td><td>-0.03936472</td><td> 0.36880110</td><td>-2.566551  </td><td>-1.675044  </td><td>-1.082050  </td><td>-0.9652184 </td><td>-1.83696600</td><td>2          </td></tr>\n",
       "\t<tr><th scope=row>V6</th><td> 0.45881630</td><td>-2.875286  </td><td> 0.1358412 </td><td> 0.4053984 </td><td>-2.0826470 </td><td> 0.13784710</td><td>1.733530   </td><td> 0.39642440</td><td> 0.04583342</td><td>0.3520643  </td><td>⋯          </td><td>-1.10201800</td><td>-1.545994  </td><td>-0.65778000</td><td> 0.39008070</td><td>-1.660205  </td><td>-1.651302  </td><td>-1.130722  </td><td>-1.1291750 </td><td> 0.04114194</td><td>2          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & x.1 & x.2 & x.3 & x.4 & x.5 & x.6 & x.7 & x.8 & x.9 & x.10 & ⋯ & x.2300 & x.2301 & x.2302 & x.2303 & x.2304 & x.2305 & x.2306 & x.2307 & x.2308 & y\\\\\n",
       "\\hline\n",
       "\tV1 &  0.77334370 & -2.438405   & -0.4825622  & -2.7211350  & -1.2170580  &  0.82780920 & 1.342604    &  0.05704174 &  0.13356890 & 0.5654274   & ⋯           & -0.02747398 & -1.660205   &  0.58823100 & -0.46362400 & -3.952845   & -5.496768   & -1.414282   & -0.6476004  & -1.76317200 & 2          \\\\\n",
       "\tV2 & -0.07817778 & -2.415754   &  0.4127717  & -2.8251460  & -0.6262365  &  0.05448819 & 1.429498    & -0.12024860 &  0.45679170 & 0.1590529   & ⋯           & -0.24628420 & -0.836325   & -0.57128360 &  0.03478783 & -2.478130   & -3.661264   & -1.093923   & -1.2093200  & -0.82439550 & 2          \\\\\n",
       "\tV3 & -0.08446916 & -1.649739   & -0.2413075  & -2.8752860  & -0.8894054  & -0.02747398 & 1.159300    &  0.01567648 &  0.19194180 & 0.4965847   & ⋯           &  0.02498525 & -1.059872   & -0.40376660 & -0.67865270 & -2.939352   & -2.736450   & -1.965399   & -0.8058680  & -1.13943400 & 2          \\\\\n",
       "\tV4 &  0.96561400 & -2.380547   &  0.6252965  & -1.7412560  & -0.8453664  &  0.94968680 & 1.093801    &  0.81973580 & -0.28462010 & 0.9947322   & ⋯           &  0.35711480 & -1.893128   &  0.25510720 &  0.16330860 & -1.021929   & -2.077843   & -1.127629   &  0.3315315  & -2.17948300 & 2          \\\\\n",
       "\tV5 &  0.07566390 & -1.728785   &  0.8526265  &  0.2726953  & -1.8413700  &  0.32793590 & 1.251219    &  0.77144990 &  0.03091710 & 0.2783133   & ⋯           &  0.06175340 & -2.273998   & -0.03936472 &  0.36880110 & -2.566551   & -1.675044   & -1.082050   & -0.9652184  & -1.83696600 & 2          \\\\\n",
       "\tV6 &  0.45881630 & -2.875286   &  0.1358412  &  0.4053984  & -2.0826470  &  0.13784710 & 1.733530    &  0.39642440 &  0.04583342 & 0.3520643   & ⋯           & -1.10201800 & -1.545994   & -0.65778000 &  0.39008070 & -1.660205   & -1.651302   & -1.130722   & -1.1291750  &  0.04114194 & 2          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "   x.1         x.2       x.3        x.4        x.5        x.6         x.7     \n",
       "V1  0.77334370 -2.438405 -0.4825622 -2.7211350 -1.2170580  0.82780920 1.342604\n",
       "V2 -0.07817778 -2.415754  0.4127717 -2.8251460 -0.6262365  0.05448819 1.429498\n",
       "V3 -0.08446916 -1.649739 -0.2413075 -2.8752860 -0.8894054 -0.02747398 1.159300\n",
       "V4  0.96561400 -2.380547  0.6252965 -1.7412560 -0.8453664  0.94968680 1.093801\n",
       "V5  0.07566390 -1.728785  0.8526265  0.2726953 -1.8413700  0.32793590 1.251219\n",
       "V6  0.45881630 -2.875286  0.1358412  0.4053984 -2.0826470  0.13784710 1.733530\n",
       "   x.8         x.9         x.10      ⋯ x.2300      x.2301    x.2302     \n",
       "V1  0.05704174  0.13356890 0.5654274 ⋯ -0.02747398 -1.660205  0.58823100\n",
       "V2 -0.12024860  0.45679170 0.1590529 ⋯ -0.24628420 -0.836325 -0.57128360\n",
       "V3  0.01567648  0.19194180 0.4965847 ⋯  0.02498525 -1.059872 -0.40376660\n",
       "V4  0.81973580 -0.28462010 0.9947322 ⋯  0.35711480 -1.893128  0.25510720\n",
       "V5  0.77144990  0.03091710 0.2783133 ⋯  0.06175340 -2.273998 -0.03936472\n",
       "V6  0.39642440  0.04583342 0.3520643 ⋯ -1.10201800 -1.545994 -0.65778000\n",
       "   x.2303      x.2304    x.2305    x.2306    x.2307     x.2308      y\n",
       "V1 -0.46362400 -3.952845 -5.496768 -1.414282 -0.6476004 -1.76317200 2\n",
       "V2  0.03478783 -2.478130 -3.661264 -1.093923 -1.2093200 -0.82439550 2\n",
       "V3 -0.67865270 -2.939352 -2.736450 -1.965399 -0.8058680 -1.13943400 2\n",
       "V4  0.16330860 -1.021929 -2.077843 -1.127629  0.3315315 -2.17948300 2\n",
       "V5  0.36880110 -2.566551 -1.675044 -1.082050 -0.9652184 -1.83696600 2\n",
       "V6  0.39008070 -1.660205 -1.651302 -1.130722 -1.1291750  0.04114194 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train <- data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain)) \n",
    "head(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the cost argument is small, then the mar- gins will be wide and many support vectors will be on the margin or will violate the margin. When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "svm(formula = y ~ ., data = data_train, kernel = \"linear\", cost = 10)\n",
       "\n",
       "\n",
       "Parameters:\n",
       "   SVM-Type:  C-classification \n",
       " SVM-Kernel:  linear \n",
       "       cost:  10 \n",
       "      gamma:  0.0004332756 \n",
       "\n",
       "Number of Support Vectors:  58\n",
       "\n",
       " ( 20 20 11 7 )\n",
       "\n",
       "\n",
       "Number of Classes:  4 \n",
       "\n",
       "Levels: \n",
       " 1 2 3 4\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(e1071)\n",
    "out = svm(y ~ ., data=data_train, kernel=\"linear\",cost=10)\n",
    "summary(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   \n",
       "     1  2  3  4\n",
       "  1  8  0  0  0\n",
       "  2  0 23  0  0\n",
       "  3  0  0 12  0\n",
       "  4  0  0  0 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# manually compute a confusion matrix \n",
    "table(out$fitted , data_train$y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are no training errors. In fact, this is not surprising, because the large number of variables relative to the number of observations implies that it is easy to find hyperplanes that fully separate the classes. We are most interested not in the support vector classifier’s performance on the training observations, but rather its performance on the test observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>x.1</th><th scope=col>x.2</th><th scope=col>x.3</th><th scope=col>x.4</th><th scope=col>x.5</th><th scope=col>x.6</th><th scope=col>x.7</th><th scope=col>x.8</th><th scope=col>x.9</th><th scope=col>x.10</th><th scope=col>⋯</th><th scope=col>x.2300</th><th scope=col>x.2301</th><th scope=col>x.2302</th><th scope=col>x.2303</th><th scope=col>x.2304</th><th scope=col>x.2305</th><th scope=col>x.2306</th><th scope=col>x.2307</th><th scope=col>x.2308</th><th scope=col>y</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>V1</th><td> 0.1395010  </td><td>-1.1689275  </td><td> 0.5649728  </td><td>-3.366796   </td><td>-1.323132   </td><td>-0.692547360</td><td>2.327395    </td><td> 0.92370319 </td><td> 0.11216730 </td><td> 0.5097651  </td><td>⋯           </td><td>-0.9426347  </td><td>-1.210662   </td><td>-0.5887872  </td><td>-0.07042246 </td><td>-2.7838519  </td><td>-2.8404394  </td><td>-1.1609133  </td><td>-0.34305385 </td><td>-0.05551271 </td><td>3           </td></tr>\n",
       "\t<tr><th scope=row>V2</th><td> 1.1642752  </td><td>-2.0181583  </td><td> 1.1035335  </td><td>-2.165435   </td><td>-1.440117   </td><td>-0.437420279</td><td>2.661587    </td><td> 1.22401070 </td><td> 0.21050401 </td><td> 1.0455631  </td><td>⋯           </td><td>-1.5329399  </td><td>-2.385967   </td><td>-0.3896410  </td><td> 0.42278099 </td><td>-2.8167496  </td><td>-2.4224954  </td><td>-1.7226066  </td><td>-1.70374859 </td><td>-1.69990982 </td><td>2           </td></tr>\n",
       "\t<tr><th scope=row>V4</th><td> 0.8410929  </td><td> 0.2547197  </td><td>-0.2087477  </td><td>-2.148149   </td><td>-1.512765   </td><td>-1.263722809</td><td>2.946642    </td><td> 0.08782771 </td><td> 0.48291986 </td><td> 1.0630197  </td><td>⋯           </td><td>-1.8540605  </td><td>-1.541312   </td><td>-1.7737231  </td><td>-1.87993516 </td><td>-2.2652893  </td><td>-2.4057259  </td><td>-0.1763792  </td><td>-0.12874288 </td><td>-0.99641678 </td><td>4           </td></tr>\n",
       "\t<tr><th scope=row>V6</th><td> 0.6850646  </td><td>-1.9275792  </td><td>-0.2330676  </td><td>-1.640413   </td><td>-1.008954   </td><td> 0.774450632</td><td>1.617168    </td><td>-0.56792522 </td><td> 0.03662118 </td><td>-0.1017006  </td><td>⋯           </td><td>-0.2639655  </td><td>-1.966113   </td><td>-1.0861898  </td><td> 0.88591400 </td><td>-0.2485896  </td><td> 0.3858745  </td><td>-0.5081625  </td><td>-0.62698498 </td><td>-0.69936648 </td><td>2           </td></tr>\n",
       "\t<tr><th scope=row>V7</th><td>-1.9561625  </td><td>-2.2349264  </td><td> 0.2815634  </td><td>-2.695628   </td><td>-1.214697   </td><td>-1.059872460</td><td>2.498070    </td><td> 0.78019606 </td><td> 1.04158328 </td><td> 0.7275003  </td><td>⋯           </td><td>-0.6931472  </td><td>-1.846427   </td><td>-0.9934418  </td><td>-3.29413831 </td><td>-3.3326046  </td><td>-2.2827825  </td><td>-0.6566224  </td><td>-2.01215680 </td><td>-1.66865727 </td><td>1           </td></tr>\n",
       "\t<tr><th scope=row>V8</th><td>-0.2586412  </td><td>-1.6847004  </td><td> 0.1758003  </td><td>-2.323809   </td><td>-1.692276   </td><td>-0.008637193</td><td>2.302135    </td><td> 0.45577792 </td><td>-0.34249031 </td><td> 0.7165219  </td><td>⋯           </td><td>-1.2238354  </td><td>-1.140372   </td><td>-0.9524362  </td><td> 0.29401200 </td><td>-1.2053070  </td><td>-1.4575756  </td><td>-0.6550810  </td><td>-0.06049338 </td><td>-0.98056262 </td><td>3           </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & x.1 & x.2 & x.3 & x.4 & x.5 & x.6 & x.7 & x.8 & x.9 & x.10 & ⋯ & x.2300 & x.2301 & x.2302 & x.2303 & x.2304 & x.2305 & x.2306 & x.2307 & x.2308 & y\\\\\n",
       "\\hline\n",
       "\tV1 &  0.1395010   & -1.1689275   &  0.5649728   & -3.366796    & -1.323132    & -0.692547360 & 2.327395     &  0.92370319  &  0.11216730  &  0.5097651   & ⋯            & -0.9426347   & -1.210662    & -0.5887872   & -0.07042246  & -2.7838519   & -2.8404394   & -1.1609133   & -0.34305385  & -0.05551271  & 3           \\\\\n",
       "\tV2 &  1.1642752   & -2.0181583   &  1.1035335   & -2.165435    & -1.440117    & -0.437420279 & 2.661587     &  1.22401070  &  0.21050401  &  1.0455631   & ⋯            & -1.5329399   & -2.385967    & -0.3896410   &  0.42278099  & -2.8167496   & -2.4224954   & -1.7226066   & -1.70374859  & -1.69990982  & 2           \\\\\n",
       "\tV4 &  0.8410929   &  0.2547197   & -0.2087477   & -2.148149    & -1.512765    & -1.263722809 & 2.946642     &  0.08782771  &  0.48291986  &  1.0630197   & ⋯            & -1.8540605   & -1.541312    & -1.7737231   & -1.87993516  & -2.2652893   & -2.4057259   & -0.1763792   & -0.12874288  & -0.99641678  & 4           \\\\\n",
       "\tV6 &  0.6850646   & -1.9275792   & -0.2330676   & -1.640413    & -1.008954    &  0.774450632 & 1.617168     & -0.56792522  &  0.03662118  & -0.1017006   & ⋯            & -0.2639655   & -1.966113    & -1.0861898   &  0.88591400  & -0.2485896   &  0.3858745   & -0.5081625   & -0.62698498  & -0.69936648  & 2           \\\\\n",
       "\tV7 & -1.9561625   & -2.2349264   &  0.2815634   & -2.695628    & -1.214697    & -1.059872460 & 2.498070     &  0.78019606  &  1.04158328  &  0.7275003   & ⋯            & -0.6931472   & -1.846427    & -0.9934418   & -3.29413831  & -3.3326046   & -2.2827825   & -0.6566224   & -2.01215680  & -1.66865727  & 1           \\\\\n",
       "\tV8 & -0.2586412   & -1.6847004   &  0.1758003   & -2.323809    & -1.692276    & -0.008637193 & 2.302135     &  0.45577792  & -0.34249031  &  0.7165219   & ⋯            & -1.2238354   & -1.140372    & -0.9524362   &  0.29401200  & -1.2053070   & -1.4575756   & -0.6550810   & -0.06049338  & -0.98056262  & 3           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "   x.1        x.2        x.3        x.4       x.5       x.6          x.7     \n",
       "V1  0.1395010 -1.1689275  0.5649728 -3.366796 -1.323132 -0.692547360 2.327395\n",
       "V2  1.1642752 -2.0181583  1.1035335 -2.165435 -1.440117 -0.437420279 2.661587\n",
       "V4  0.8410929  0.2547197 -0.2087477 -2.148149 -1.512765 -1.263722809 2.946642\n",
       "V6  0.6850646 -1.9275792 -0.2330676 -1.640413 -1.008954  0.774450632 1.617168\n",
       "V7 -1.9561625 -2.2349264  0.2815634 -2.695628 -1.214697 -1.059872460 2.498070\n",
       "V8 -0.2586412 -1.6847004  0.1758003 -2.323809 -1.692276 -0.008637193 2.302135\n",
       "   x.8         x.9         x.10       ⋯ x.2300     x.2301    x.2302    \n",
       "V1  0.92370319  0.11216730  0.5097651 ⋯ -0.9426347 -1.210662 -0.5887872\n",
       "V2  1.22401070  0.21050401  1.0455631 ⋯ -1.5329399 -2.385967 -0.3896410\n",
       "V4  0.08782771  0.48291986  1.0630197 ⋯ -1.8540605 -1.541312 -1.7737231\n",
       "V6 -0.56792522  0.03662118 -0.1017006 ⋯ -0.2639655 -1.966113 -1.0861898\n",
       "V7  0.78019606  1.04158328  0.7275003 ⋯ -0.6931472 -1.846427 -0.9934418\n",
       "V8  0.45577792 -0.34249031  0.7165219 ⋯ -1.2238354 -1.140372 -0.9524362\n",
       "   x.2303      x.2304     x.2305     x.2306     x.2307      x.2308      y\n",
       "V1 -0.07042246 -2.7838519 -2.8404394 -1.1609133 -0.34305385 -0.05551271 3\n",
       "V2  0.42278099 -2.8167496 -2.4224954 -1.7226066 -1.70374859 -1.69990982 2\n",
       "V4 -1.87993516 -2.2652893 -2.4057259 -0.1763792 -0.12874288 -0.99641678 4\n",
       "V6  0.88591400 -0.2485896  0.3858745 -0.5081625 -0.62698498 -0.69936648 2\n",
       "V7 -3.29413831 -3.3326046 -2.2827825 -0.6566224 -2.01215680 -1.66865727 1\n",
       "V8  0.29401200 -1.2053070 -1.4575756 -0.6550810 -0.06049338 -0.98056262 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_test=data.frame(x=Khan$xtest , y=as.factor(Khan$ytest))\n",
    "head(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         \n",
       "pred_test 1 2 3 4\n",
       "        1 3 0 0 0\n",
       "        2 0 6 2 0\n",
       "        3 0 0 4 0\n",
       "        4 0 0 0 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run the predict function \n",
    "pred_test=predict(out, newdata=data_test)\n",
    "# output confusion matrix \n",
    "table(pred_test, data_test$y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two errors! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 3.3",
   "language": "R",
   "name": "ir33"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
